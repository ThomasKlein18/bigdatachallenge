{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['run', 'walk', 'stand', 'sit', 'sit-to-stand', 'stand-to-sit', \n",
    "        'stair-up', 'stair-down', 'jump-one-leg', 'jump-two-leg', 'curve-left-step', \n",
    "        'curve-right-step', 'curve-left-spin-Lfirst', 'curve-left-spin-Rfirst', \n",
    "        'curve-right-spin-Lfirst', 'curve-right-spin-Rfirst', 'lateral-shuffle-left', \n",
    "        'lateral-shuffle-right','v-cut-left-Lfirst', 'v-cut-left-Rfirst', 'v-cut-right-Lfirst', 'v-cut-right-Rfirst']\n",
    "\n",
    "sensors = ['EMG1', 'EMG2', 'EMG3', 'EMG4', 'Microphone', 'ACC upper X', 'ACC upper Y','ACC upper Z', 'Goniometer X',\n",
    "          'ACC lower X', 'ACC lower Y', 'ACC lower Z', 'Goniometer Y', 'Gyro upper X', 'Gyro upper Y', 'Gyro upper Z',\n",
    "          'Gyro lower X', 'Gyro lower Y', 'Gyro lower Z']\n",
    "\n",
    "variance_sensors = ['EMG1', 'EMG2', 'EMG3', 'EMG4', 'Microphone']\n",
    "\n",
    "smooth_sensors = ['ACC upper X', 'ACC upper Y','ACC upper Z', 'Goniometer X','ACC lower X', 'ACC lower Y', \n",
    "                  'ACC lower Z', 'Goniometer Y', 'Gyro upper X', 'Gyro upper Y', 'Gyro upper Z', 'Gyro lower X', \n",
    "                  'Gyro lower Y', 'Gyro lower Z']\n",
    "\n",
    "data_path = \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bbdc_2019_Bewegungsdaten/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data, windowsize, std):\n",
    "    kernel = signal.gaussian(windowsize, std=std)\n",
    "    kernel /= np.sum(kernel)\n",
    "    return np.convolve(data, kernel, 'valid')\n",
    "\n",
    "def variance_filter(data, windowsize):\n",
    "    half = windowsize//2\n",
    "    res = np.zeros(data.shape[0]-windowsize)\n",
    "    for i in range(half,len(data)-half):\n",
    "        res[i-half] = np.std(data[i-half:i+half])\n",
    "    maxi = np.max(res)\n",
    "    if(maxi == 0):\n",
    "        print(\"max:\",maxi)\n",
    "    return res / maxi\n",
    "\n",
    "def sample(data, num_samples):\n",
    "    samples = [int(sample) for sample in np.linspace(0, data.shape[0]-1, num_samples)]\n",
    "    return data[samples]\n",
    "    \n",
    "def smooth_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 1d-numpy array of length timestep:\n",
    "    \"\"\"\n",
    "    smoothed = smooth(data,200,50)\n",
    "    normalized = (smoothed-np.mean(smoothed))/np.max(smoothed)\n",
    "    return sample(normalized, num_samples)\n",
    "\n",
    "def variance_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 1d-numpy array of length timesteps\n",
    "    \"\"\"\n",
    "    var_data = smooth(variance_filter(data,windowsize=100),windowsize=100,std=25)\n",
    "    return sample(var_data, num_samples)\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_feature_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 2d-numpy array of shape [timesteps, sensors]\n",
    "    \n",
    "    \"\"\"\n",
    "    features = []\n",
    "        \n",
    "    for sensor in variance_sensors:\n",
    "        features.append(variance_extractor(data[:,sensors.index(sensor)], num_samples))\n",
    "        \n",
    "    if(np.isnan(np.array(features)).any()):\n",
    "        raise ValueError(\"Error in variance\")\n",
    "        \n",
    "    for sensor in smooth_sensors:\n",
    "        features.append(smooth_extractor(data[:,sensors.index(sensor)], num_samples))\n",
    "        \n",
    "    if(np.isnan(np.array(features)).any()):\n",
    "        raise ValueError(\"Error in smooth\")\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(featurelist, label):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message from the list of features and the label, where\n",
    "    every element in the featurelist is actually a sequence=ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    feature = {}\n",
    "    for i in range(len(featurelist)):\n",
    "        feature['feature'+str(i)] = tf.train.Feature(float_list=tf.train.FloatList(value=list(featurelist[i])))\n",
    "        #_float_feature(featurelist[i])\n",
    "    feature['label'] = _int64_feature(label)\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(in_file, outfile):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(in_file)\n",
    "    featurelist = []\n",
    "    \n",
    "    with tf.python_io.TFRecordWriter(outfile+\".tfrecords\") as writer:\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if(index % 100 == 0):\n",
    "                print(row['Datafile'])\n",
    "            if(row['Label'] in classes):\n",
    "                path = row['Datafile']\n",
    "                data = pd.read_csv(data_path+path).values\n",
    "                if np.isnan(data).any():\n",
    "                    raise ValueError(\"Encountered NaN\")\n",
    "\n",
    "                label = classes.index(row['Label'])\n",
    "                extracted_featurelist = recurrent_feature_extractor(data, 80)\n",
    "                if np.isnan(np.array(extracted_featurelist)).any():\n",
    "                    raise ValueError(\"Encountered NaN after processing\")\n",
    "                # this is where the fun begins: extracted_featurelist is a 19-element-list of numpy arrays of length 80\n",
    "                # We need to get that into a tf.train.Example, which we can then serialize to string and \n",
    "                # write to a tfrecords-file.\n",
    "                serialized_example = serialize_example(extracted_featurelist, label)\n",
    "\n",
    "                writer.write(serialized_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject02/Subject02_Aufnahme000.csv\n",
      "Subject02/Subject02_Aufnahme100.csv\n",
      "Subject02/Subject02_Aufnahme200.csv\n",
      "Subject02/Subject02_Aufnahme300.csv\n",
      "Subject02/Subject02_Aufnahme400.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Encountered NaN after processing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a5f4036f04e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_creator(data_path+\"train.csv\",\n\u001b[0;32m----> 2\u001b[0;31m                 \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/recurrent/rec_features\")\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-78341c40ad67>\u001b[0m in \u001b[0;36mdataset_creator\u001b[0;34m(in_file, outfile)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mextracted_featurelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_featurelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encountered NaN after processing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;31m# this is where the fun begins: extracted_featurelist is a 19-element-list of numpy arrays of length 80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# We need to get that into a tf.train.Example, which we can then serialize to string and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Encountered NaN after processing"
     ]
    }
   ],
   "source": [
    "dataset_creator(data_path+\"train.csv\",\n",
    "                \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/recurrent/rec_features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
