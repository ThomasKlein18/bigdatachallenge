{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['run', 'walk', 'stand', 'sit', 'sit-to-stand', 'stand-to-sit', \n",
    "        'stair-up', 'stair-down', 'jump-one-leg', 'jump-two-leg', 'curve-left-step', \n",
    "        'curve-right-step', 'curve-left-spin-Lfirst', 'curve-left-spin-Rfirst', \n",
    "        'curve-right-spin-Lfirst', 'curve-right-spin-Rfirst', 'lateral-shuffle-left', \n",
    "        'lateral-shuffle-right','v-cut-left-Lfirst', 'v-cut-left-Rfirst', 'v-cut-right-Lfirst', 'v-cut-right-Rfirst']\n",
    "\n",
    "sensors = ['EMG1', 'EMG2', 'EMG3', 'EMG4', 'Microphone', 'ACC upper X', 'ACC upper Y','ACC upper Z', 'Goniometer X',\n",
    "          'ACC lower X', 'ACC lower Y', 'ACC lower Z', 'Goniometer Y', 'Gyro upper X', 'Gyro upper Y', 'Gyro upper Z',\n",
    "          'Gyro lower X', 'Gyro lower Y', 'Gyro lower Z']\n",
    "\n",
    "data_path = \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bbdc_2019_Bewegungsdaten/\"\n",
    "\n",
    "variance_analysis_params = {'EMG1':(100, 0.5, 20,  6),\n",
    "                            'EMG2':(200, 0.5, 20, 10),\n",
    "                            'EMG3':(200, 0.2, 20,  6),\n",
    "                            'EMG4':(200, 0.5, 20,  6),\n",
    "                            'Microphone': (200, 0.6, 20, 4),\n",
    "                            'ACC upper X':(200, 0.2, 20, 6),\n",
    "                            'ACC upper Y':(200, 0.5, 20, 5),\n",
    "                            'ACC upper Z':(200, 0.5, 20, 5),\n",
    "                            'Goniometer X':(200, 0.5, 20, 6),\n",
    "                            'ACC lower X':(200, 0.2, 20, 6),\n",
    "                            'ACC lower Y':(200, 0.2, 20, 6),\n",
    "                            'ACC lower Z':(200, 0.2, 20, 6),\n",
    "                            'Goniometer Y':(200, 0.5, 20, 6),\n",
    "                            'Gyro upper X':(200, 0.5, 20, 6),\n",
    "                            'Gyro upper Y':(200, 0.5, 20, 6),\n",
    "                            'Gyro upper Z':(200, 0.5, 20, 6),\n",
    "                            'Gyro lower X':(200, 0.5, 20, 6),\n",
    "                            'Gyro lower Y':(200, 0.5, 20, 6),\n",
    "                            'Gyro lower Z':(200, 0.5, 20, 6)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_filter(data, windowsize):\n",
    "    \"\"\"\n",
    "    Filters a signal by sliding a window over it, under which the variance is calculated.\n",
    "    At every time point t, the method outputs the variance of the original signal within a window around t\n",
    "    \"\"\"\n",
    "    half = windowsize//2\n",
    "    res = np.zeros(data.shape[0]-windowsize)\n",
    "    for i in range(half,len(data)-half):\n",
    "        res[i-half] = np.std(data[i-half:i+half])\n",
    "    return res#/np.max(res)\n",
    "\n",
    "def smooth(data, windowsize, std):\n",
    "    kernel = signal.gaussian(windowsize, std=std)\n",
    "    kernel /= np.sum(kernel)\n",
    "    return np.convolve(data, kernel, 'valid')\n",
    "\n",
    "def derivative_filter(data, windowsize, std):\n",
    "    lowpass = smooth(data, windowsize, std)\n",
    "    deriv = ndimage.sobel(lowpass, 0)\n",
    "    return deriv\n",
    "    \n",
    "\n",
    "def peakfinder(data, h_thresh, w_thresh, n):\n",
    "    \"\"\"\n",
    "    Finds peaks in a spectrum. Returns the first six peaks as \n",
    "    \n",
    "    location1, location2, ...  location6\n",
    "    height1,   height2,   ...  height6\n",
    "    width1,    width2,    ...  width6\n",
    "    \n",
    "    The location is normalized so that the location of the first peak corresponds to zero.\n",
    "    \n",
    "    arguments: \n",
    "        data = 1d-array (of variance intensity values)\n",
    "        h_thresh = the threshold for the minimum height a peak needs to have\n",
    "        w_thresh = the threshold for the minimum width a peak needs to have\n",
    "        n = the number of peaks to extract\n",
    "    \"\"\"\n",
    "    #print(data)\n",
    "    if h_thresh == None:\n",
    "        h_thresh = np.mean(data)\n",
    "        #print(\"max: \",np.max(data))\n",
    "        #print(\"had to change height, new height:\",h_thresh)\n",
    "    locations, properties = signal.find_peaks(data, height=h_thresh, width=w_thresh)\n",
    "    heights = properties['peak_heights']\n",
    "    #widths = properties['width_heights']\n",
    "    left = properties['left_ips']\n",
    "    right = properties['right_ips']\n",
    "    results = np.stack((locations,heights))#,left,right))\n",
    "    #print(results)\n",
    "    \n",
    "    # if no peaks were found, return empty array\n",
    "    if results.size == 0:\n",
    "        #print(\"returning empty array\")\n",
    "        return np.zeros((2,n))\n",
    "    \n",
    "    # subtract location of first peak to shift everything (this doesn't make sense)\n",
    "    #results[0,:] = results[0,:] - results[0,0]\n",
    "    \n",
    "    # sort descending by peak height\n",
    "    results = results[:,(-results[1,:]).argsort()]\n",
    "    \n",
    "    # select biggest n peaks\n",
    "    results = results[:,0:n]\n",
    "    \n",
    "    # rounding to 3 decimal places\n",
    "    #results = np.round(results,3)\n",
    "\n",
    "    # if not enough peaks found, fill up with zeros\n",
    "    if results.shape[1] != n:\n",
    "        results = np.pad(results,((0,0),(0,n-results.shape[1])),'constant', constant_values=0)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_analysis(data, windowsize, h_thresh, w_thresh, n):\n",
    "    \"\"\"\n",
    "    data = 1d-numpy array of shape [timesteps]\n",
    "    \"\"\"\n",
    "    vardata = variance_filter(data, windowsize)\n",
    "    res = peakfinder(vardata, h_thresh, w_thresh, 3) # instead of n\n",
    "    return res.flatten()\n",
    "\n",
    "def derivative_feature_extractor(data, sensor):\n",
    "    derdata = derivative_filter(data, windowsize=200, std=50)\n",
    "    res = peakfinder(derdata, h_thresh=None, w_thresh=20, n=5)\n",
    "    return res.flatten()\n",
    "    \n",
    "def variance_feature_extractor(data, sensor):\n",
    "    \"\"\"\n",
    "    Calls variance analysis with appropr\n",
    "    \"\"\"\n",
    "    windowsize, h_thresh, w_thresh, n = variance_analysis_params[sensor]\n",
    "    \n",
    "    return variance_analysis(data, windowsize, h_thresh, w_thresh, n)\n",
    "\n",
    "def extract_airborne(data):\n",
    "    num_segments = 5\n",
    "    \n",
    "    airborne = data[:,4] # select airborne column\n",
    "    airborne = np.abs(airborne - np.mean(airborne)) # center data around zero and take absolute value\n",
    "    airborne /= np.max(airborne) # normalize between 0 and 1\n",
    "    airborne = ndimage.filters.gaussian_filter1d(airborne, 40) # smooth data with gaussian\n",
    "    segments = np.array_split(airborne, num_segments)\n",
    "    \n",
    "    maximums = np.array([np.max(seg) for seg in segments])\n",
    "    stds = np.array([np.std(seg) for seg in segments])\n",
    "    \n",
    "    features = np.array([maximums, stds])\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(data):\n",
    "    \"\"\"\n",
    "    data = 2d-numpy array of shape [timesteps, sensors]\n",
    "    \n",
    "    \"\"\"\n",
    "    features = []\n",
    "    #for idx,sensor in enumerate(['EMG1','Microphone']):\n",
    "    #    features = features + list(variance_feature_extractor(data[:,idx], sensor))\n",
    "        \n",
    "    for idx,sensor in enumerate(['Goniometer X']):\n",
    "        features = features + list(derivative_feature_extractor(data[:,idx], sensor))\n",
    "    \n",
    "    features + list(extract_airborne(data))\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        features.append(np.mean(data[:,sensors.index(sensor)]))\n",
    "        features.append(np.std(data[:,sensors.index(sensor)]))\n",
    "        \n",
    "    #features.append(np.std(data[:,sensors.index('Goniometer X')]))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(in_file, target_path, outfile):\n",
    "    df = pd.read_csv(in_file)\n",
    "    featurelist = []\n",
    "    for index, row in df.iterrows():\n",
    "        if(index % 100 == 0):\n",
    "            print(row['Datafile'])\n",
    "        if(row['Label'] in classes):\n",
    "            path = row['Datafile']\n",
    "            data = pd.read_csv(data_path+path).values\n",
    "            features = feature_extractor(data)\n",
    "            features.append(classes.index(row['Label']))\n",
    "            featurelist.append(np.array(features))\n",
    "    features = np.array(featurelist)\n",
    "    print(\"Shape of features:\",features.shape)\n",
    "    features[:,0:-1] = (features[:,0:-1]-np.mean(features[:,0:-1],axis=0))/np.max(features[:,0:-1],axis=0)\n",
    "    \n",
    "    indices = np.random.randint(low=0, high=features.shape[0], size=features.shape[0]//10)\n",
    "    testset = np.take(features, indices, axis=0)\n",
    "    features = np.delete(features, indices, axis=0)\n",
    "    \n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "        \n",
    "    odf = pd.DataFrame(features)\n",
    "    odf.to_csv(target_path+outfile+\".csv\")\n",
    "    \n",
    "    tdf = pd.DataFrame(testset)\n",
    "    tdf.to_csv(target_path+outfile+\"_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject02/Subject02_Aufnahme000.csv\n",
      "Subject02/Subject02_Aufnahme100.csv\n",
      "Subject02/Subject02_Aufnahme200.csv\n",
      "Subject02/Subject02_Aufnahme300.csv\n",
      "Subject02/Subject02_Aufnahme400.csv\n",
      "Subject03/Subject03_Aufnahme060.csv\n",
      "Subject03/Subject03_Aufnahme160.csv\n",
      "Subject03/Subject03_Aufnahme260.csv\n",
      "Subject03/Subject03_Aufnahme360.csv\n",
      "Subject04/Subject04_Aufnahme020.csv\n",
      "Subject04/Subject04_Aufnahme120.csv\n",
      "Subject04/Subject04_Aufnahme220.csv\n",
      "Subject04/Subject04_Aufnahme320.csv\n",
      "Subject04/Subject04_Aufnahme420.csv\n",
      "Subject05/Subject05_Aufnahme082.csv\n",
      "Subject05/Subject05_Aufnahme182.csv\n",
      "Subject05/Subject05_Aufnahme282.csv\n",
      "Subject05/Subject05_Aufnahme382.csv\n",
      "Subject06/Subject06_Aufnahme044.csv\n",
      "Subject06/Subject06_Aufnahme144.csv\n",
      "Subject06/Subject06_Aufnahme244.csv\n",
      "Subject06/Subject06_Aufnahme344.csv\n",
      "Subject06/Subject06_Aufnahme444.csv\n",
      "Subject07/Subject07_Aufnahme089.csv\n",
      "Subject07/Subject07_Aufnahme189.csv\n",
      "Subject07/Subject07_Aufnahme289.csv\n",
      "Subject07/Subject07_Aufnahme389.csv\n",
      "Subject08/Subject08_Aufnahme049.csv\n",
      "Subject08/Subject08_Aufnahme149.csv\n",
      "Subject08/Subject08_Aufnahme249.csv\n",
      "Subject08/Subject08_Aufnahme349.csv\n",
      "Subject09/Subject09_Aufnahme021.csv\n",
      "Subject09/Subject09_Aufnahme121.csv\n",
      "Subject09/Subject09_Aufnahme221.csv\n",
      "Subject09/Subject09_Aufnahme321.csv\n",
      "Subject09/Subject09_Aufnahme421.csv\n",
      "Subject11/Subject11_Aufnahme082.csv\n",
      "Subject11/Subject11_Aufnahme182.csv\n",
      "Subject11/Subject11_Aufnahme282.csv\n",
      "Subject11/Subject11_Aufnahme382.csv\n",
      "Subject12/Subject12_Aufnahme050.csv\n",
      "Subject12/Subject12_Aufnahme150.csv\n",
      "Subject12/Subject12_Aufnahme250.csv\n",
      "Subject12/Subject12_Aufnahme350.csv\n",
      "Subject13/Subject13_Aufnahme010.csv\n",
      "Subject13/Subject13_Aufnahme110.csv\n",
      "Subject13/Subject13_Aufnahme210.csv\n",
      "Subject13/Subject13_Aufnahme310.csv\n",
      "Subject13/Subject13_Aufnahme410.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject16/Subject16_Aufnahme070.csv\n",
      "Subject16/Subject16_Aufnahme170.csv\n",
      "Subject17/Subject17_Aufnahme015.csv\n",
      "Subject17/Subject17_Aufnahme115.csv\n",
      "Subject17/Subject17_Aufnahme215.csv\n",
      "Subject17/Subject17_Aufnahme315.csv\n",
      "Subject17/Subject17_Aufnahme415.csv\n",
      "Subject18/Subject18_Aufnahme075.csv\n",
      "Subject18/Subject18_Aufnahme175.csv\n",
      "Subject18/Subject18_Aufnahme275.csv\n",
      "Subject18/Subject18_Aufnahme375.csv\n",
      "Subject19/Subject19_Aufnahme039.csv\n",
      "Subject19/Subject19_Aufnahme139.csv\n",
      "Subject19/Subject19_Aufnahme239.csv\n",
      "Subject19/Subject19_Aufnahme339.csv\n",
      "Subject19/Subject19_Aufnahme439.csv\n",
      "Shape of features: (6385, 31)\n"
     ]
    }
   ],
   "source": [
    "dataset_creator(data_path+\"train.csv\",\n",
    "                \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/cafeteria/\",\n",
    "                \"minifeat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def convert_csv_to_tfrecords(file,outfile):\n",
    "    csv = pd.read_csv(file).values\n",
    "\n",
    "    def serialize_example(featurelist):\n",
    "        \"\"\"\n",
    "        Creates a tf.Example message ready to be written to a file.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a dictionary mapping the feature name to the type of list\n",
    "        feature = {}\n",
    "        for i in range(csv.shape[1]-1):\n",
    "            feature['feature'+str(i)] = _float_feature(featurelist[i])\n",
    "        feature['label'] = _int64_feature(int(featurelist[-1]))\n",
    "\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "    with tf.python_io.TFRecordWriter(outfile+\".tfrecords\") as writer:\n",
    "        for row in csv:\n",
    "            example = serialize_example(row)\n",
    "            writer.write(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_csv_to_tfrecords(\"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/cafeteria/minifeat.csv\",\"cafeteria/minifeat\")\n",
    "\n",
    "convert_csv_to_tfrecords(\"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/cafeteria/minifeat_test.csv\",\"cafeteria/minifeat_test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
