{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['run', 'walk', 'stand', 'sit', 'sit-to-stand', 'stand-to-sit', \n",
    "        'stair-up', 'stair-down', 'jump-one-leg', 'jump-two-leg', 'curve-left-step', \n",
    "        'curve-right-step', 'curve-left-spin-Lfirst', 'curve-left-spin-Rfirst', \n",
    "        'curve-right-spin-Lfirst', 'curve-right-spin-Rfirst', 'lateral-shuffle-left', \n",
    "        'lateral-shuffle-right','v-cut-left-Lfirst', 'v-cut-left-Rfirst', 'v-cut-right-Lfirst', 'v-cut-right-Rfirst']\n",
    "\n",
    "sensors = ['EMG1', 'EMG2', 'EMG3', 'EMG4', 'Microphone', 'ACC upper X', 'ACC upper Y','ACC upper Z', 'Goniometer X',\n",
    "          'ACC lower X', 'ACC lower Y', 'ACC lower Z', 'Goniometer Y', 'Gyro upper X', 'Gyro upper Y', 'Gyro upper Z',\n",
    "          'Gyro lower X', 'Gyro lower Y', 'Gyro lower Z']\n",
    "\n",
    "variance_sensors = ['EMG1', 'EMG2', 'EMG3', 'EMG4', 'Microphone']\n",
    "\n",
    "smooth_sensors = ['ACC upper X', 'ACC upper Y','ACC upper Z', 'Goniometer X','ACC lower X', 'ACC lower Y', \n",
    "                  'ACC lower Z', 'Goniometer Y', 'Gyro upper X', 'Gyro upper Y', 'Gyro upper Z', 'Gyro lower X', \n",
    "                  'Gyro lower Y', 'Gyro lower Z']\n",
    "\n",
    "data_path = \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bbdc_2019_Bewegungsdaten/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data, windowsize, std):\n",
    "    kernel = signal.gaussian(windowsize, std=std)\n",
    "    kernel /= np.sum(kernel)\n",
    "    return np.convolve(data, kernel, 'valid')\n",
    "\n",
    "def variance_filter(data, windowsize):\n",
    "    half = windowsize//2\n",
    "    res = np.zeros(data.shape[0]-windowsize)\n",
    "    for i in range(half,len(data)-half):\n",
    "        res[i-half] = np.std(data[i-half:i+half])\n",
    "    maxi = np.max(res)\n",
    "    if(maxi == 0):\n",
    "        maxi = 0.00001\n",
    "    return res / maxi\n",
    "\n",
    "def sample(data, num_samples):\n",
    "    samples = [int(sample) for sample in np.linspace(0, data.shape[0]-1, num_samples)]\n",
    "    return data[samples]\n",
    "    \n",
    "def smooth_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 1d-numpy array of length timestep:\n",
    "    \"\"\"\n",
    "    smoothed = smooth(data,200,50)\n",
    "    smax = np.max(smoothed)\n",
    "    if smax == 0:\n",
    "        smax = 0.00001\n",
    "    normalized = (smoothed-np.mean(smoothed))/smax\n",
    "    return sample(normalized, num_samples)\n",
    "\n",
    "def variance_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 1d-numpy array of length timesteps\n",
    "    \"\"\"\n",
    "    var_data = smooth(variance_filter(data,windowsize=100),windowsize=100,std=25)\n",
    "    return sample(var_data, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_feature_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 2d-numpy array of shape [timesteps, sensors]\n",
    "    \n",
    "    \"\"\"\n",
    "    features = []\n",
    "        \n",
    "    for sensor in variance_sensors:\n",
    "        features.append(variance_extractor(data[:,sensors.index(sensor)], num_samples))\n",
    "        \n",
    "    if(np.isnan(np.array(features)).any()):\n",
    "        raise ValueError(\"Error in variance\")\n",
    "        \n",
    "    for sensor in smooth_sensors:\n",
    "        features.append(smooth_extractor(data[:,sensors.index(sensor)], num_samples))\n",
    "        \n",
    "    if(np.isnan(np.array(features)).any()):\n",
    "        raise ValueError(\"Error in smooth\")\n",
    "        \n",
    "    return features\n",
    "\n",
    "def threaded_recurrent_feature_extractor(data, num_samples):\n",
    "    \"\"\"\n",
    "    data = 2d-numpy array of shape [timesteps, sensors]\n",
    "    \n",
    "    \"\"\"\n",
    "    pool = ThreadPool(8)\n",
    "    \n",
    "    variance_sequences = []\n",
    "    smooth_sequences = []\n",
    "    \n",
    "    for sensor in variance_sensors:\n",
    "        variance_sequences.append(data[:,sensors.index(sensor)])\n",
    "        \n",
    "    for sensor in smooth_sensors:\n",
    "        smooth_sequences.append(data[:,sensors.index(sensor)])\n",
    "        \n",
    "    var_results = pool.starmap(variance_extractor, zip(variance_sequences, itertools.repeat(num_samples)))\n",
    "    \n",
    "    if(np.isnan(np.array(var_results)).any()):\n",
    "        raise ValueError(\"NaN after variance feature extraction\")\n",
    "        \n",
    "    smo_results = pool.starmap(smooth_extractor, zip(smooth_sequences, itertools.repeat(num_samples)))\n",
    "        \n",
    "    if(np.isnan(np.array(smo_results)).any()):\n",
    "        raise ValueError(\"NaN after smoothing variance extraction\")\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return var_results + smo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(featurelist, label):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message from the list of features and the label, where\n",
    "    every element in the featurelist is actually a sequence=ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    feature = {}\n",
    "    for i in range(len(featurelist)):\n",
    "        feature['feature'+str(i)] = tf.train.Feature(float_list=tf.train.FloatList(value=list(featurelist[i])))\n",
    "        #_float_feature(featurelist[i])\n",
    "    feature['label'] = _int64_feature(label)\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(file, train_name, test_name, percentage=10):\n",
    "    \"\"\"\n",
    "    Splits the file that contains the original dataset in two files, one for training and one for testing.\n",
    "    \n",
    "    file = the original file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    headers = list(df)\n",
    "    files = df.values\n",
    "    \n",
    "    indices = np.random.randint(low=0, high=files.shape[0], size=files.shape[0]//percentage)\n",
    "    \n",
    "    testset = np.take(files, indices, axis=0)\n",
    "    files = np.delete(files, indices, axis=0)\n",
    "    \n",
    "    odf = pd.DataFrame(files)\n",
    "    odf.columns = headers\n",
    "    odf.to_csv(train_name+\".csv\")\n",
    "    \n",
    "    tdf = pd.DataFrame(testset)\n",
    "    tdf.columns = headers\n",
    "    tdf.to_csv(test_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(in_file, outfile):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(in_file)\n",
    "    \n",
    "    with tf.python_io.TFRecordWriter(outfile+\".tfrecords\") as writer:\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if(index % 100 == 0):\n",
    "                print(\"Digesting\",row['Datafile'])\n",
    "            if(row['Label'] in classes):\n",
    "                path = row['Datafile']\n",
    "                data = pd.read_csv(data_path+path).values\n",
    "\n",
    "                label = classes.index(row['Label'])\n",
    "                extracted_featurelist = recurrent_feature_extractor(data, 80)\n",
    "\n",
    "                # this is where the fun begins: extracted_featurelist is a 19-element-list of  np-arrays of length 80\n",
    "                # We need to get that into a tf.train.Example, which we can then serialize to string and \n",
    "                # write to a tfrecords-file.\n",
    "                serialized_example = serialize_example(extracted_featurelist, label)\n",
    "\n",
    "                writer.write(serialized_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digesting Subject02/Subject02_Aufnahme000.csv\n",
      "Digesting Subject02/Subject02_Aufnahme112.csv\n",
      "Digesting Subject02/Subject02_Aufnahme226.csv\n",
      "Digesting Subject02/Subject02_Aufnahme337.csv\n",
      "Digesting Subject03/Subject03_Aufnahme010.csv\n",
      "Digesting Subject03/Subject03_Aufnahme120.csv\n",
      "Digesting Subject03/Subject03_Aufnahme234.csv\n",
      "Digesting Subject03/Subject03_Aufnahme344.csv\n",
      "Digesting Subject04/Subject04_Aufnahme014.csv\n",
      "Digesting Subject04/Subject04_Aufnahme128.csv\n",
      "Digesting Subject04/Subject04_Aufnahme234.csv\n",
      "Digesting Subject04/Subject04_Aufnahme342.csv\n",
      "Digesting Subject05/Subject05_Aufnahme019.csv\n",
      "Digesting Subject05/Subject05_Aufnahme136.csv\n",
      "Digesting Subject05/Subject05_Aufnahme247.csv\n",
      "Digesting Subject05/Subject05_Aufnahme352.csv\n",
      "Digesting Subject06/Subject06_Aufnahme025.csv\n",
      "Digesting Subject06/Subject06_Aufnahme130.csv\n",
      "Digesting Subject06/Subject06_Aufnahme243.csv\n",
      "Digesting Subject06/Subject06_Aufnahme353.csv\n",
      "Digesting Subject07/Subject07_Aufnahme012.csv\n",
      "Digesting Subject07/Subject07_Aufnahme118.csv\n",
      "Digesting Subject07/Subject07_Aufnahme231.csv\n",
      "Digesting Subject07/Subject07_Aufnahme334.csv\n",
      "Digesting Subject08/Subject08_Aufnahme001.csv\n",
      "Digesting Subject08/Subject08_Aufnahme113.csv\n",
      "Digesting Subject08/Subject08_Aufnahme225.csv\n",
      "Digesting Subject08/Subject08_Aufnahme335.csv\n",
      "Digesting Subject09/Subject09_Aufnahme013.csv\n",
      "Digesting Subject09/Subject09_Aufnahme124.csv\n",
      "Digesting Subject09/Subject09_Aufnahme232.csv\n",
      "Digesting Subject09/Subject09_Aufnahme341.csv\n",
      "Digesting Subject11/Subject11_Aufnahme013.csv\n",
      "Digesting Subject11/Subject11_Aufnahme126.csv\n",
      "Digesting Subject11/Subject11_Aufnahme236.csv\n",
      "Digesting Subject11/Subject11_Aufnahme348.csv\n",
      "Digesting Subject12/Subject12_Aufnahme031.csv\n",
      "Digesting Subject12/Subject12_Aufnahme149.csv\n",
      "Digesting Subject12/Subject12_Aufnahme255.csv\n",
      "Digesting Subject12/Subject12_Aufnahme373.csv\n",
      "Digesting Subject13/Subject13_Aufnahme040.csv\n",
      "Digesting Subject13/Subject13_Aufnahme151.csv\n",
      "Digesting Subject13/Subject13_Aufnahme260.csv\n",
      "Digesting Subject13/Subject13_Aufnahme369.csv\n",
      "Digesting Subject16/Subject16_Aufnahme035.csv\n",
      "Digesting Subject16/Subject16_Aufnahme144.csv\n",
      "Digesting Subject16/Subject16_Aufnahme253.csv\n",
      "Digesting Subject17/Subject17_Aufnahme104.csv\n",
      "Digesting Subject17/Subject17_Aufnahme216.csv\n",
      "Digesting Subject17/Subject17_Aufnahme327.csv\n",
      "Digesting Subject17/Subject17_Aufnahme439.csv\n",
      "Digesting Subject18/Subject18_Aufnahme108.csv\n",
      "Digesting Subject18/Subject18_Aufnahme223.csv\n",
      "Digesting Subject18/Subject18_Aufnahme328.csv\n",
      "Digesting Subject18/Subject18_Aufnahme435.csv\n",
      "Digesting Subject19/Subject19_Aufnahme111.csv\n",
      "Digesting Subject19/Subject19_Aufnahme227.csv\n",
      "Digesting Subject19/Subject19_Aufnahme338.csv\n",
      "Digesting Subject08/Subject08_Aufnahme230.csv\n",
      "Digesting Subject06/Subject06_Aufnahme367.csv\n",
      "Digesting Subject11/Subject11_Aufnahme387.csv\n",
      "Digesting Subject19/Subject19_Aufnahme051.csv\n",
      "Digesting Subject03/Subject03_Aufnahme222.csv\n",
      "Digesting Subject08/Subject08_Aufnahme038.csv\n",
      "Digesting Subject04/Subject04_Aufnahme300.csv\n"
     ]
    }
   ],
   "source": [
    "#split_dataset(data_path+\"train.csv\", \"rec_train\", \"rec_test\")\n",
    "\n",
    "dataset_creator(\"rec_train.csv\",\n",
    "                \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/recurrent/detail_rec_train\")\n",
    "\n",
    "dataset_creator(\"rec_test.csv\",\n",
    "                \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/recurrent/detail_rec_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digesting Subject01/Subject01_Aufnahme000.csv\n",
      "Digesting Subject01/Subject01_Aufnahme100.csv\n",
      "Digesting Subject01/Subject01_Aufnahme200.csv\n",
      "Digesting Subject01/Subject01_Aufnahme300.csv\n",
      "Digesting Subject01/Subject01_Aufnahme400.csv\n",
      "Digesting Subject10/Subject10_Aufnahme060.csv\n",
      "Digesting Subject10/Subject10_Aufnahme160.csv\n",
      "Digesting Subject10/Subject10_Aufnahme260.csv\n",
      "Digesting Subject10/Subject10_Aufnahme360.csv\n",
      "Digesting Subject14/Subject14_Aufnahme042.csv\n",
      "Digesting Subject14/Subject14_Aufnahme142.csv\n",
      "Digesting Subject14/Subject14_Aufnahme242.csv\n",
      "Digesting Subject14/Subject14_Aufnahme342.csv\n",
      "Digesting Subject15/Subject15_Aufnahme002.csv\n",
      "Digesting Subject15/Subject15_Aufnahme102.csv\n",
      "Digesting Subject15/Subject15_Aufnahme202.csv\n",
      "Digesting Subject15/Subject15_Aufnahme302.csv\n",
      "Digesting Subject15/Subject15_Aufnahme402.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_creator(\"abgabe1.csv\",\n",
    "                \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/recurrent/abgabe\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738\n"
     ]
    }
   ],
   "source": [
    "dataset_creator(\"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bbdc_2019_Bewegungsdaten/train.csv\",\n",
    "                \"/Users/thomasklein/Projects/BremenBigDataChallenge2019/bigdatachallenge/recurrent/full80\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
